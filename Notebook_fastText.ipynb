{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we do some experiments using fasttext library for text classification, without directly designing any neural network. As discussed in README file, fasttext can be used as a powerful baseline in many different text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a class that transforms train/test data into raw text format, so that it can be used by fasttext for training/validation in next stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_text_generator( sample_train_iterator , output_file_name ):\n",
    "    with open( output_file_name , \"w\") as F:\n",
    "            for i in sample_train_iterator:\n",
    "                    temp_text = \" \" + \" \".join( i.__dict__[\"text\"] )\n",
    "                    temp_label = i.__dict__[\"label\"]\n",
    "                    F.write( \"__label__\" + temp_label + temp_text + \"\\n\" )\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get started by splitting the IMDB sentiment analysis dataset into train/test datasets, and then generating the corresponding text files using the previously defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = get_tokenizer(\"basic_english\")\n",
    "TEXT = torchtext.data.Field( sequential=True, tokenize=tokenize, lower=True )\n",
    "LABEL = torchtext.data.LabelField( )#dtype=torch.float )\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL )\n",
    "\n",
    "\n",
    "\n",
    "training_text_generator( train_data , \"train_file.txt\" )\n",
    "training_text_generator( test_data , \"test_file.txt\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of pos/neg labels is equal (both in training and test set). Let's get some info about our training and test datasets, using their transformed text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25000  6790783 33812043 train_file.txt\n",
      "   25000  6639743 33019346 test_file.txt\n"
     ]
    }
   ],
   "source": [
    "!wc train_file.txt #25K training samples\n",
    "!wc test_file.txt  # 25K test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffling and splitting the training dataset into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -o train_file.txt  train_file.txt\n",
    "!head -n 22000 train_file.txt > imdb.train\n",
    "!tail -n 3000 train_file.txt > imdb.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets traing our first classifier using fasttext default supervised mode parameters and see how it performs on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__neg': {'precision': 0.8749163879598663,\n",
       "  'recall': 0.8708388814913449,\n",
       "  'f1score': 0.8728728728728729},\n",
       " '__label__pos': {'precision': 0.8710963455149502,\n",
       "  'recall': 0.8751668891855807,\n",
       "  'f1score': 0.8731268731268731}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.train_supervised( input=\"imdb.train\")\n",
    "model.test_label(\"imdb.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset is relatively small, it might be a good idea to increase #epoch (default is 5), and to also finetune learning rate to achieve better performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__neg': {'precision': 0.8946322067594433,\n",
       "  'recall': 0.8988015978695073,\n",
       "  'f1score': 0.8967120557954168},\n",
       " '__label__pos': {'precision': 0.898054996646546,\n",
       "  'recall': 0.8938584779706275,\n",
       "  'f1score': 0.8959518233522917}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.train_supervised( input=\"imdb.train\" , epoch=20 , lr=0.1)\n",
    "model.test_label(\"imdb.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we also consider word bi-grams in model training (As it introduces more parameters to be trained, #epoch also needs to be changed accordingly (grid-search can help us to find the best hyper-parameters for each architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__neg': {'precision': 0.8961892247043364,\n",
       "  'recall': 0.9081225033288948,\n",
       "  'f1score': 0.9021164021164021},\n",
       " '__label__pos': {'precision': 0.9066305818673883,\n",
       "  'recall': 0.8945260347129506,\n",
       "  'f1score': 0.9005376344086021}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.train_supervised( input=\"imdb.train\" , epoch=30 , lr=0.1 , wordNgrams=2)\n",
    "model.test_label(\"imdb.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in only few steps, we were able to achieve a reasonable performance on the validations set, now let's see how our best model performs on the held-out test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__neg': {'precision': 0.8934042384316215,\n",
       "  'recall': 0.90048,\n",
       "  'f1score': 0.8969281644687039},\n",
       " '__label__pos': {'precision': 0.8996855092331264,\n",
       "  'recall': 0.89256,\n",
       "  'f1score': 0.8961085900164653}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_label(\"test_file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with a good performance using just a simple architecture of embedding averaging of input sentence. Performance can also be enhanced with a more extensive grid-search, and with also deploying ideas like using pretrained-embeddings, instead of training from scratch (which e.g. helps us to mitigate overfitting during training)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

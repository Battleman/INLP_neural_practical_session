{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is a modified version of the code in the following repo:\n",
    "#                      https://github.com/prakashpandey9/Text-Classification-Pytorch\n",
    "## We made several changes to make it compatible with the latest version of torchtext, and to also add some more\n",
    "## features to it to improve the final performance. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Here is the licence of the source code:\n",
    "\n",
    "\n",
    "# MIT License\n",
    "# Copyright (c) 2018 Prakash Pandey\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the dataset loader function which is written using torchtext. As mentioned in the README file, here we focus on sentiment detection on IMDB reviews dataset. As we can see in this cell, torchtext has separate module for this dataset which makes it even easier for us to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset( BS, emb_DIM, test_sen=None):\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
    "    Field : A class that stores information about the way of preprocessing\n",
    "    fix_length : Here we are using fix_length which will pad each sequence to have a fix length of 200.\n",
    "                 \n",
    "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in\n",
    "                  the train_data to an idx and then after it will use GloVe word embedding to map the \n",
    "                  index to the corresponding word embedding.\n",
    "                  \n",
    "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim)\n",
    "                    containing the pre-trained word embeddings.\n",
    "                    \n",
    "    BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize\n",
    "                     the amount of padding needed.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True,\n",
    "                                              include_lengths=True, batch_first=True, fix_length=200)\n",
    "    LABEL = data.LabelField( dtype=torch.float )\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL )\n",
    "    \n",
    "    #Loading Glove pretrained vectors with dimension of DIM\n",
    "    TEXT.build_vocab( train_data, vectors=GloVe(name='6B', dim=emb_DIM ))\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "\n",
    "    # Further splitting of training_data to create new training_data & validation_data\n",
    "    train_data, valid_data = train_data.split() \n",
    "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                       batch_size=BS, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "\n",
    "    '''Alternatively we can also use the default configurations'''\n",
    "    # train_iter, test_iter = datasets.IMDB.iters(batch_size=32)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model architecture which is a LSTM on the input word embeddings, and then its last hidden state is passed to a fully connected layer to map to [\"pos\",\"neg\"] classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sentence, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "\n",
    "        \"\"\"\n",
    "        device_temp = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        ''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
    "        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
    "        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size) ).to(device_temp) # Initial hidden state of the LSTM\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size) ).to(device_temp) # Initial cell state of the LSTM\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size) ).to(device_temp)\n",
    "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size) ).to(device_temp)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        final_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent the gradients from exploding (which might happen in a RNN architecture), we defind a clip_gradient class that clips all gradients to some predefined value.\n",
    "Moreover, it's a good practice to define separate train_model and eval_model classes that are responsibe for batch_training steps and also model evaluation in the middle of training to monitor the overall performance. \n",
    "Defining these two classes separately will make the code cleaner, and also helps you to debug the whole architecture more easily once you encounter an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "    \n",
    "def train_model(model, train_iter, epoch, optimizer, BS):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    \n",
    "    \n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()        \n",
    "        if (text.size()[0] is not BS):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target = Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                    text = text.cuda()\n",
    "                    target = target.cuda()            \n",
    "            prediction = model(text)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading IMDB reviews dataset (with their sentiment label), and also the pretrained Glove word embeddings. Here we use 300-dimensional word embeddings, but you might also consider other dimensions in your hyper-parameter search phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 251639\n",
      "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
      "Label Length: 2\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "embedding_length = 300\n",
    "\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset( batch_size , embedding_length )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 100, Training Loss: 0.6736, Training Accuracy:  59.38%\n",
      "Epoch: 1, Idx: 200, Training Loss: 0.6764, Training Accuracy:  53.12%\n",
      "Epoch: 1, Idx: 300, Training Loss: 0.7234, Training Accuracy:  56.25%\n",
      "Epoch: 1, Idx: 400, Training Loss: 0.6768, Training Accuracy:  50.00%\n",
      "Epoch: 1, Idx: 500, Training Loss: 0.6837, Training Accuracy:  59.38%\n",
      "Epoch: 01, Train Loss: 0.679, Train Acc: 55.15%, Val. Loss: 0.685150, Val. Acc: 55.77%\n",
      "Epoch: 2, Idx: 100, Training Loss: 0.7238, Training Accuracy:  53.12%\n",
      "Epoch: 2, Idx: 200, Training Loss: 0.7021, Training Accuracy:  56.25%\n",
      "Epoch: 2, Idx: 300, Training Loss: 0.5586, Training Accuracy:  71.88%\n",
      "Epoch: 2, Idx: 400, Training Loss: 0.6955, Training Accuracy:  59.38%\n",
      "Epoch: 2, Idx: 500, Training Loss: 0.7518, Training Accuracy:  37.50%\n",
      "Epoch: 02, Train Loss: 0.657, Train Acc: 61.26%, Val. Loss: 0.691126, Val. Acc: 50.73%\n",
      "Epoch: 3, Idx: 100, Training Loss: 0.6897, Training Accuracy:  56.25%\n",
      "Epoch: 3, Idx: 200, Training Loss: 0.6977, Training Accuracy:  53.12%\n",
      "Epoch: 3, Idx: 300, Training Loss: 0.6538, Training Accuracy:  62.50%\n",
      "Epoch: 3, Idx: 400, Training Loss: 0.6945, Training Accuracy:  50.00%\n",
      "Epoch: 3, Idx: 500, Training Loss: 0.6813, Training Accuracy:  62.50%\n",
      "Epoch: 03, Train Loss: 0.679, Train Acc: 55.28%, Val. Loss: 0.658858, Val. Acc: 60.88%\n",
      "Epoch: 4, Idx: 100, Training Loss: 0.4074, Training Accuracy:  87.50%\n",
      "Epoch: 4, Idx: 200, Training Loss: 0.5899, Training Accuracy:  71.88%\n",
      "Epoch: 4, Idx: 300, Training Loss: 0.2816, Training Accuracy:  90.62%\n",
      "Epoch: 4, Idx: 400, Training Loss: 0.4605, Training Accuracy:  81.25%\n",
      "Epoch: 4, Idx: 500, Training Loss: 0.5039, Training Accuracy:  78.12%\n",
      "Epoch: 04, Train Loss: 0.490, Train Acc: 77.18%, Val. Loss: 0.426066, Val. Acc: 80.36%\n",
      "Epoch: 5, Idx: 100, Training Loss: 0.6227, Training Accuracy:  71.88%\n",
      "Epoch: 5, Idx: 200, Training Loss: 0.3062, Training Accuracy:  87.50%\n",
      "Epoch: 5, Idx: 300, Training Loss: 0.3443, Training Accuracy:  87.50%\n",
      "Epoch: 5, Idx: 400, Training Loss: 0.2249, Training Accuracy:  84.38%\n",
      "Epoch: 5, Idx: 500, Training Loss: 0.3443, Training Accuracy:  90.62%\n",
      "Epoch: 05, Train Loss: 0.393, Train Acc: 82.50%, Val. Loss: 0.438395, Val. Acc: 80.92%\n",
      "Epoch: 6, Idx: 100, Training Loss: 0.4119, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 200, Training Loss: 0.2457, Training Accuracy:  90.62%\n",
      "Epoch: 6, Idx: 300, Training Loss: 0.2635, Training Accuracy:  90.62%\n",
      "Epoch: 6, Idx: 400, Training Loss: 0.2243, Training Accuracy:  93.75%\n",
      "Epoch: 6, Idx: 500, Training Loss: 0.4597, Training Accuracy:  71.88%\n",
      "Epoch: 06, Train Loss: 0.352, Train Acc: 84.29%, Val. Loss: 0.393406, Val. Acc: 82.75%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.3244, Training Accuracy:  90.62%\n",
      "Epoch: 7, Idx: 200, Training Loss: 0.3092, Training Accuracy:  84.38%\n",
      "Epoch: 7, Idx: 300, Training Loss: 0.3926, Training Accuracy:  84.38%\n",
      "Epoch: 7, Idx: 400, Training Loss: 0.2091, Training Accuracy:  96.88%\n",
      "Epoch: 7, Idx: 500, Training Loss: 0.2171, Training Accuracy:  90.62%\n",
      "Epoch: 07, Train Loss: 0.307, Train Acc: 86.97%, Val. Loss: 0.376382, Val. Acc: 83.10%\n",
      "Epoch: 8, Idx: 100, Training Loss: 0.3471, Training Accuracy:  84.38%\n",
      "Epoch: 8, Idx: 200, Training Loss: 0.4560, Training Accuracy:  84.38%\n",
      "Epoch: 8, Idx: 300, Training Loss: 0.1596, Training Accuracy:  90.62%\n",
      "Epoch: 8, Idx: 400, Training Loss: 0.1645, Training Accuracy:  93.75%\n",
      "Epoch: 8, Idx: 500, Training Loss: 0.2802, Training Accuracy:  84.38%\n",
      "Epoch: 08, Train Loss: 0.267, Train Acc: 89.07%, Val. Loss: 0.409207, Val. Acc: 82.79%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0fc9ee202fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d3c2b2cabe6f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iter, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnum_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_corrects\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mclip_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reza/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reza/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-5\n",
    "output_size = 2\n",
    "hidden_size = 256\n",
    "\n",
    "EPOCH = 20\n",
    "ckpt_saving_path = \"model_ckpt.pth\"\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMClassifier( batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters() ) , lr = learning_rate )\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch, optim, batch_size)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "    \n",
    "    if val_acc > best_val_acc :\n",
    "            torch.save( model.state_dict(), ckpt_saving_path )\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "model.load_state_dict( torch.load(ckpt_saving_path) )    \n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss, test_acc = eval_model(model, test_iter)\n",
    "        print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        \n",
    "# best validation score after 20 epochs : 0.8102\n",
    "# best performing model through learning (on validation set) accuracy on test set: 0.8067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can also test different hyper parameters on validation set to find a better model that is able to boost performance on held-out test dataset.\n",
    "Moreover, we could think of adding some modules like dropout in the architecture to evaluate its effect on performance, or even maybe replacing RNN by CNN, as it shows promising result in sentiment analysis, however these methods are completely out-of-scope for this introductory session, and we leave it for more advanced ML courses.\n",
    "\n",
    "You can also use Google Colab service to accelerate your model training by using GPUs. The code is modified to be compatible with GPUs, and once a GPU-enabled environment is used, model is trained on GPU as well. Check their service if you have trouble with training due to training time:\n",
    "#### https://colab.research.google.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
